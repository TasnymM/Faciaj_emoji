{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXaiziln9tvn",
        "outputId": "13fbd69a-42dc-49bb-8e18-1fc0cc99b5dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "dataset_path = os.path.join(path, \"train\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObxHnqYy_4YF",
        "outputId": "8f28c1dd-f01b-416b-9d9e-531d50fabb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/fer2013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chemins des données\n",
        "train_path = os.path.join(path, \"train\")\n",
        "test_path = os.path.join(path, \"test\")\n"
      ],
      "metadata": {
        "id": "50MJf0XxADUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifiez d'abord les noms réels de vos dossiers\n",
        "print(os.listdir(train_path))\n",
        "\n",
        "# Puis mettez à jour le dictionnaire emotion_classes en conséquence\n",
        "emotion_classes = {\n",
        "    0: \"angry\",\n",
        "    1: \"disgust\",\n",
        "    2: \"fear\",\n",
        "    3: \"happy\",\n",
        "    4: \"sad\",\n",
        "    5: \"surprise\",\n",
        "    6: \"neutral\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dyXRl9BADWv",
        "outputId": "14271db8-3d96-42cb-a029-475bc1d032be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['surprise', 'fear', 'angry', 'neutral', 'sad', 'disgust', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations pour les images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "M2g57X0JADZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, data_path, transform=None):\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        self.classes = sorted(os.listdir(data_path))  # Tri pour consistance\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.images = []\n",
        "\n",
        "        for emotion in self.classes:\n",
        "            emotion_path = os.path.join(data_path, emotion)\n",
        "            for img in os.listdir(emotion_path):\n",
        "                self.images.append((os.path.join(emotion_path, img), self.class_to_idx[emotion]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.images[idx]\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "KemlOmHQADcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement des données\n",
        "train_dataset = EmotionDataset(train_path, transform=transform)\n",
        "test_dataset = EmotionDataset(test_path, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "v55M7Zx-6qwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle CNN simple\n",
        "class EmotionCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(EmotionCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = x.view(-1, 64 * 12 * 12)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "SU4NBH3I6q7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation du modèle\n",
        "model = EmotionCNN(num_classes=7).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "L06uhaMj7z1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction d'entraînement\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_acc = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Ud8QfyOs7z6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n"
      ],
      "metadata": {
        "id": "2fmNRIgF6rEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction d'évaluation\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "lKGktXASAy0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluation\n",
        "test_accuracy = evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "xPFCgSesA2ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour prédire une émotion à partir d'une image\n",
        "def predict_emotion(image_path, model, transform):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path)\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        emotion = emotion_classes[predicted.item()]\n",
        "\n",
        "    return emotion"
      ],
      "metadata": {
        "id": "DDchloTVA2xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple d'utilisation avec une image uploadée\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Upload de l'image\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    # Prédiction de l'émotion\n",
        "    emotion = predict_emotion(filename, model, transform)\n",
        "    print(f\"L'émotion prédite est: {emotion}\")\n"
      ],
      "metadata": {
        "id": "cHkc6VTVA-dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_QKnT_OBHen"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}